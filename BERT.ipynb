{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad2503-38be-40fc-8b4e-5e95880891cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers umap-learn matplotlib seaborn scikit-learn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d5958-0617-47c7-a7bd-47aae6cca4fc",
   "metadata": {},
   "source": [
    "## PIP installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a3687e-0b15-4389-abf4-542748193731",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b70000-b33c-4235-9465-1d351cae19fd",
   "metadata": {},
   "source": [
    "## WebScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588cd98-c8ec-4b87-b347-3d979d0260ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.web_scraper import WebScraper\n",
    "\n",
    "scraper = WebScraper()\n",
    "text = scraper.scrape_url(\"https://www.mckinsey.com/capabilities/operations/our-insights/future-proofing-the-supply-chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d70a08-75a8-4ce2-8518-c02b703ca083",
   "metadata": {},
   "source": [
    "## PDF Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f0f93-7208-4190-8bc9-d8c851e46c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pdf_extractor import extract_multiple_pdfs,process_pdf_folder\n",
    "\n",
    "#processed_count = process_pdf_folder()\n",
    "processed_count = process_pdf_folder(input_folder=\"sc_input\", output_folder=\"sc_output\", processed_folder=\"sc_processed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee264374-c245-47d2-99b4-d93d3d269e9b",
   "metadata": {},
   "source": [
    "## Process all Text files by referencing their directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86e821-7fe7-4eff-9ead-917c1fbe1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.process_text_files\n",
    "importlib.reload(utils.process_text_files)\n",
    "\n",
    "\n",
    "from utils.process_text_files import process_files\n",
    "\n",
    "try:\n",
    "    sentences, labels, numeric_labels = process_files([\"sc_output\", \"fin_output\", \"scraped_content\"])\n",
    "    \n",
    "    print(f\"Found {len(sentences)} risk-related sentences\")\n",
    "    print(\"\\nFirst few examples:\")\n",
    "    for i in range(min(50, len(sentences))):\n",
    "        print(f\"{i+1}. [{labels[i]}] {sentences[i][:100]}...\")\n",
    "\n",
    "    print(f\"Found {len(sentences)} risk-related sentences\")\n",
    "    \n",
    "    # Count sentences for each label\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    print(\"\\nSentence count by label:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"{label}: {count}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    print(\"Make sure you've run the scraper first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96401d28-09f9-4d63-b809-74b719dd2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe7e96-c8ea-409a-8d0d-6015bee20898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embeddings(text_list):\n",
    "    all_cls_layers = []\n",
    "\n",
    "    for text in text_list:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        hidden_states = outputs.hidden_states  # (13 layers, batch, seq_len, hidden_dim)\n",
    "        cls_per_layer = [layer[0][0].numpy() for layer in hidden_states]  # Get [CLS] at each layer\n",
    "        all_cls_layers.append(cls_per_layer)\n",
    "\n",
    "    return np.array(all_cls_layers)  # shape: (samples, layers, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9db6a4-b9b0-4286-939b-1a3dfc70639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_embeddings = get_cls_embeddings(sentences)\n",
    "# cls_embeddings.shape = (N samples, 13 layers, 768 dims)\n",
    "label_map = {\"Legal\": 0, \"Supply Chain\": 1, \"Financial\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ef3997-6491-4bdc-b083-cbe85a5d7c4d",
   "metadata": {},
   "source": [
    "## Plotting UMAP of all sentences in Supply chain and finances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e8aba-b918-4a5e-bdf2-0e60758aa1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_umap_per_layer(embeddings, labels, label_names, sentences, layers=[1, 4, 8, 12]):\n",
    "    for layer in layers:\n",
    "        reducer = umap.UMAP(random_state=42)\n",
    "        layer_embeds = embeddings[:, layer, :]\n",
    "        umap_proj = reducer.fit_transform(layer_embeds)\n",
    "\n",
    "        # Create a DataFrame for easier plotting\n",
    "        df = pd.DataFrame({\n",
    "            'x': umap_proj[:, 0],\n",
    "            'y': umap_proj[:, 1],\n",
    "            'label': [label_names[l] for l in labels],\n",
    "            'sentence': sentences\n",
    "        })\n",
    "\n",
    "        fig = px.scatter(\n",
    "            df, x='x', y='y',\n",
    "            color='label',\n",
    "            hover_data={'sentence': True, 'x': False, 'y': False},\n",
    "            title=f\"UMAP of Layer {layer} CLS Embeddings\"\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=6))\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b215a03-90da-4376-837d-e4a0a365d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_umap_per_layer(cls_embeddings, numeric_labels, list(label_map.keys()),sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9016539-2a42-4408-bb5e-1ae4f6c4d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a804504-53f1-40f0-ba3b-1e18c2ec6f64",
   "metadata": {},
   "source": [
    "## Plot UMAP of BERT Topic of all sentences in Supply Chain and Finances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047092dc-75bd-42b4-83fc-048fb860485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP of all BETtopic != -1\n",
    "    }\n",
    "    topic_words[-1] = \"Outlier\"  # Handle outliers\n",
    "\n",
    "    # Step 3: Build readable topic labels\n",
    "    topic_labels = [topic_words[t] for t in topics]\n",
    "\n",
    "    # Step 4: Plot UMAP per selected BERT layer\n",
    "    for layer in layers:\n",
    "        reducer = umap.UMAP(random_state=42)\n",
    "        layer_embeds = cls_embeddings[:, layer, :]\n",
    "        umap_proj = reducer.fit_transform(layer_embeds)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'x': umap_proj[:, 0],\n",
    "            'y': umap_proj[:, 1],\n",
    "            'label': topic_labels,\n",
    "            'sentence': sentences\n",
    "        })\n",
    "\n",
    "        fig = px.scatter(\n",
    "            df, x='x', y='y',\n",
    "            color='label',\n",
    "            hover_data={'sentence': True, 'x': False, 'y': False},\n",
    "            title=f\"UMAP of Layer {layer} CLS Embeddings (BERTopic Keywords)\"\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=6))\n",
    "        fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9fd5c-328a-4c85-96d6-ebbfcdf3e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bertopic_umap(cls_embeddings, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c978546-831c-42af-b1f7-104877a41ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_bertopic_filtered_umap(cls_embeddings, sentences, layers=[1, 4, 8, 12]):\n",
    "    # Step 1: Fit BERTopic\n",
    "    topic_model = BERTopic(language=\"english\")\n",
    "    topics, _ = topic_model.fit_transform(sentences)\n",
    "\n",
    "    # Step 2: Define keywords for filtering\n",
    "    supply_keywords = {\"shipment\", \"delay\", \"vendor\", \"logistics\", \"port\", \"customs\", \"invoice\", \"raw\", \"supply\"}\n",
    "    financial_keywords = {\"revenue\", \"cost\", \"loss\", \"liability\", \"inflation\", \"price\", \"profit\", \"expense\"}\n",
    "\n",
    "    # Step 3: Extract topic words\n",
    "    topic_words = {}\n",
    "    topic_filter = set()\n",
    "    for topic in set(topics):\n",
    "        if topic == -1:\n",
    "            continue\n",
    "        words = [word for word, _ in topic_model.get_topic(topic)[:5]]\n",
    "        topic_words[topic] = \", \".join(words)\n",
    "\n",
    "        # Check if topic has relevant keywords\n",
    "        if any(w in supply_keywords or w in financial_keywords for w in words):\n",
    "            topic_filter.add(topic)\n",
    "\n",
    "    topic_words[-1] = \"Outlier\"  # Fallback\n",
    "    topic_filter.add(-1)         # Optional: include outliers\n",
    "\n",
    "    # Step 4: Label sentences and filter\n",
    "    filtered_indices = [i for i, t in enumerate(topics) if t in topic_filter]\n",
    "    filtered_embeddings = cls_embeddings[filtered_indices]\n",
    "    filtered_sentences = [sentences[i] for i in filtered_indices]\n",
    "    filtered_labels = [topic_words[topics[i]] for i in filtered_indices]\n",
    "\n",
    "    # Step 5: Plot for each layer\n",
    "    for layer in layers:\n",
    "        reducer = umap.UMAP(random_state=42)\n",
    "        layer_embeds = filtered_embeddings[:, layer, :]\n",
    "        umap_proj = reducer.fit_transform(layer_embeds)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'x': umap_proj[:, 0],\n",
    "            'y': umap_proj[:, 1],\n",
    "            'label': filtered_labels,\n",
    "            'sentence': filtered_sentences\n",
    "        })\n",
    "\n",
    "        fig = px.scatter(\n",
    "            df, x='x', y='y',\n",
    "            color='label',\n",
    "            hover_data={'sentence': True, 'x': False, 'y': False},\n",
    "            title=f\"UMAP of Layer {layer} CLS Embeddings (Filtered Topics)\"\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=6))\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178a1ff-031d-4e56-9514-755f13be9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bertopic_filtered_umap(cls_embeddings, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4b4ea-2bff-44b6-b705-ad470a38b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from bertopic import BERTopic\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 👉 INPUT your real data here:\n",
    "# cls_embeddings: shape (num_samples, num_layers, 768)\n",
    "# sentences: List of strings\n",
    "\n",
    "# Example dummy data:\n",
    "# cls_embeddings = np.random.rand(100, 13, 768)\n",
    "# sentences = [\"your sentence 1\", \"your sentence 2\", ..., \"your sentence N\"]\n",
    "\n",
    "def calculate_cluster_tightness(cls_embeddings, sentences, layers=[12]):\n",
    "    # Step 1: Fit BERTopic\n",
    "    topic_model = BERTopic(language=\"english\")\n",
    "    topics, _ = topic_model.fit_transform(sentences)\n",
    "\n",
    "    # Step 2: Define filtering keywords\n",
    "    supply_keywords = {\"shipment\", \"delay\", \"vendor\", \"logistics\", \"port\", \"customs\", \"invoice\", \"raw\", \"supply\"}\n",
    "    financial_keywords = {\"revenue\", \"cost\", \"loss\", \"liability\", \"inflation\", \"price\", \"profit\", \"expense\"}\n",
    "\n",
    "    # Step 3: Get topic keywords and filter\n",
    "    topic_words = {}\n",
    "    topic_filter = set()\n",
    "    for topic in set(topics):\n",
    "        if topic == -1:\n",
    "            continue\n",
    "        words = [word for word, _ in topic_model.get_topic(topic)[:5]]\n",
    "        topic_words[topic] = \", \".join(words)\n",
    "        if any(w in supply_keywords or w in financial_keywords for w in words):\n",
    "            topic_filter.add(topic)\n",
    "    topic_words[-1] = \"Outlier\"\n",
    "    topic_filter.add(-1)\n",
    "\n",
    "    # Step 4: Filter sentences/embeddings by topic\n",
    "    filtered_indices = [i for i, t in enumerate(topics) if t in topic_filter]\n",
    "    filtered_embeddings = cls_embeddings[filtered_indices]\n",
    "    filtered_sentences = [sentences[i] for i in filtered_indices]\n",
    "    filtered_labels = [topic_words[topics[i]] for i in filtered_indices]\n",
    "\n",
    "    # Step 5: Calculate tightness for each layer and topic\n",
    "    results = []\n",
    "    for layer in layers:\n",
    "        layer_embeds = filtered_embeddings[:, layer, :]  # (N, 768)\n",
    "        df = pd.DataFrame(layer_embeds)\n",
    "        df['label'] = filtered_labels\n",
    "\n",
    "        for label in df['label'].unique():\n",
    "            cluster_points = df[df['label'] == label].drop(columns='label').values\n",
    "            if len(cluster_points) < 2:\n",
    "                continue\n",
    "            centroid = cluster_points.mean(axis=0)\n",
    "            dists = cdist(cluster_points, [centroid], metric='euclidean')\n",
    "            mean_dist = dists.mean()\n",
    "            results.append({\n",
    "                'Topic': label,\n",
    "                'Layer': layer,\n",
    "                'Avg_Distance_to_Centroid': mean_dist,\n",
    "                'Num_Points': len(cluster_points)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e65a9f-2014-4181-a9b8-256d76036105",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = calculate_cluster_tightness(cls_embeddings, sentences)\n",
    "print(results_df.sort_values(\"Avg_Distance_to_Centroid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf744daa-9675-4c74-b515-b60b4e6c36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef538ed0-fae8-47d7-be4b-e8a9e989570c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
